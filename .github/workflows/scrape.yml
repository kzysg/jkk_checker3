name: JKK Scraper (with Cache Update)

on:
  # schedule:
  #   - cron: '*/5 23 * * *'
  #   - cron: '*/5 0-15 * * *'
  workflow_dispatch:

jobs:
  # ----------------------------
  # â‘  ã‚¸ãƒ§ãƒ–1ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã‚“ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
  # ----------------------------
  scrape:
    runs-on: ubuntu-latest
    outputs:
      changed: ${{ steps.scraper.outputs.changed }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Chrome & ChromeDriver
        run: |
          sudo apt update
          sudo apt install -y google-chrome-stable unzip curl jq
          CHROME_VERSION=$(google-chrome --version | grep -oE '[0-9]+(\.[0-9]+){3}' | cut -d '.' -f 1)
          DRIVER_VERSION=$(curl -s "https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json" \
            | jq -r --arg v "$CHROME_VERSION" '.versions[] | select(.version | startswith($v + ".")) | .version' | head -n 1)
          DRIVER_URL=$(curl -s "https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json" \
            | jq -r --arg v "$DRIVER_VERSION" '.versions[] | select(.version == $v) | .downloads.chromedriver[] | select(.platform=="linux64").url')
          curl -Lo chromedriver.zip "$DRIVER_URL"
          unzip chromedriver.zip
          sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4 requests jq

      - name: Restore previous result cache
        id: restore-cache
        uses: actions/cache/restore@v4
        with:
          path: previous_result.txt
          key: previous-result
          restore-keys: |
            previous-result

      - name: Show cached previous result
        run: |
          echo "ğŸ“‚ Cached previous_result.txt:"
          cat previous_result.txt || echo "âš ï¸ previous_result.txt not found"

      - name: Run scraper
        id: scraper
        run: |
          python main_scrape4.py
          echo "changed=true" >> $GITHUB_OUTPUT  # å·®åˆ†é€šçŸ¥ãŒå¿…è¦ãªã¨ãã®ã¿ãƒ•ãƒ©ã‚°å‡ºåŠ›

  # ----------------------------
  # â‘¡ ã‚¸ãƒ§ãƒ–2ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¸Šæ›¸ãæ›´æ–°ï¼ˆå¸¸ã«æœ€æ–°ä¿å­˜ï¼‰
  # ----------------------------
  update-cache:
    runs-on: ubuntu-latest
    needs: scrape
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Upload new previous_result.txt to cache
        uses: actions/cache/save@v4
        with:
          path: previous_result.txt
          key: previous-result
