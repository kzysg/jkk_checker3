name: JKK Scraper (cache-safe)
concurrency:
  group: jkk-scraper
  cancel-in-progress: true

on:
  workflow_dispatch:
  # schedule:
  #   - cron: '*/5 0-15 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          # ✅ Chrome & ChromeDriver を自動で同期インストール
      - name: Install matching ChromeDriver
        run: |
          sudo apt update
          sudo apt install -y google-chrome-stable unzip curl
          CHROME_VERSION=$(google-chrome --version | grep -oE '[0-9]+(\.[0-9]+){3}' | cut -d '.' -f 1)
          echo "Detected Chrome major version: $CHROME_VERSION"
          DRIVER_VERSION=$(curl -s "https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json" \
            | jq -r --arg v "$CHROME_VERSION" '.versions[] | select(.version | startswith($v + ".")) | .version' | head -n 1)
          echo "Installing ChromeDriver version: $DRIVER_VERSION"
          DRIVER_URL=$(curl -s "https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json" \
            | jq -r --arg v "$DRIVER_VERSION" '.versions[] | select(.version == $v) | .downloads.chromedriver[] | select(.platform=="linux64").url')
          curl -Lo chromedriver.zip "$DRIVER_URL"
          unzip chromedriver.zip
          sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver
 
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4 requests jq

      - name: Set Discord Webhook
        run: echo "DISCORD_WEBHOOK_URL=${{ secrets.DISCORD_WEBHOOK_URL }}" >> $GITHUB_ENV

      - name: Restore previous cache
        id: restore
        uses: actions/cache/restore@v4
        with:
          path: previous_result.txt
          key: previous-result
          restore-keys: |
            previous-result-

      - name: Run scraper
        run: python main_scrape4.py

      - name: Upload file to artifact
        uses: actions/upload-artifact@v4
        with:
          name: previous_result
          path: previous_result.txt

  update-cache:
    runs-on: ubuntu-latest
    needs: scrape
    steps:
      - name: Download file from artifact
        uses: actions/download-artifact@v4
        with:
          name: previous_result

      - name: Save updated cache
        uses: actions/cache/save@v4
        with:
          path: previous_result.txt
          key: previous-result-${{ github.run_id }}
